# Machine Learning Visualization Techniques

This project focuses on using visualization techniques to analyze high-dimensional data and improve the performance of machine learning models. The goal is to classify sounds generated by interactions with textured surfaces using optimized feature selection and preprocessing.

---

## Table of Contents
1. [Introduction](#introduction)
2. [Methodology](#methodology)
3. [Key Findings](#key-findings)
4. [Conclusion](#conclusion)

---

## Introduction

Data visualization is a crucial step in building effective machine learning systems. It helps uncover patterns and relationships within the data, guiding better feature selection and preprocessing. This project explores how techniques like PCA, t-SNE, and UMAP can highlight separations between classes in audio data.

Audio features were extracted using transformations such as:
- **Fast Fourier Transform (FFT)**
- **Discrete Cosine Transform (DCT)**
- **Cepstrum**

These features were then paired with windowing functions like Hamming, Hann, Boxcar, and Blackman-Harris to test their effectiveness in clustering data for classification.

---

## Methodology

### Classifier
A simple K-Nearest Neighbors (KNN) algorithm was used with \( K = 7 \). This algorithm classifies new data points based on their similarity to existing labeled data.

### Visualization Techniques
Several dimensionality reduction methods were applied to visualize the high-dimensional features:
- **PCA (Principal Component Analysis)**
- **t-SNE (t-distributed Stochastic Neighbor Embedding)**
- **UMAP (Uniform Manifold Approximation and Projection)**
- **Isomap**
- **LLE (Locally Linear Embedding)**

### Parameters
The key parameters used in feature extraction included:
- **Window Size**: 212 samples  
- **Step Size**: \( \frac{212}{24} \) samples  
- **Decimation**: 2  
- **Feature Functions**: FFT, DCT, Cepstrum, etc.  
- **Window Functions**: Hamming, Hann, Boxcar, Blackman-Harris  

---

## Key Findings

| Finding                                | Description                                                                 |
|----------------------------------------|-----------------------------------------------------------------------------|
| **Unclustered Graphs**                 | Poor clustering correlates with low model accuracy.                         |
| **Well-Clustered Graphs**              | Not always indicative of the highest accuracy.                              |
| **Semi-Clustered Graphs**              | Often result in the best accuracy scores.                                   |
| **Best Feature Function**              | Cepstrum delivered the highest average accuracy of 80.7%.                   |
| **Best Window Function**               | Boxcar consistently outperformed other windowing functions.                 |
| **Best Visualization Technique**       | t-SNE was the most reliable, effectively separating classes.                |

---

## Conclusion

This project demonstrates the importance of visualization in feature engineering for machine learning. Semi-clustered data often yielded better results than well-clustered data. Cepstrum features and the Boxcar window function emerged as the best-performing combination, while t-SNE proved to be the most dependable visualization method for identifying class separations.

These findings emphasize the role of visualization in optimizing machine learning pipelines, especially for complex, high-dimensional data.
