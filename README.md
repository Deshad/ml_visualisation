# Machine Learning Visualization Techniques

This project explores the use of visualization techniques to analyze features and improve the performance of machine learning models. By visualizing high-dimensional data, we aim to optimize feature selection for classifying sounds generated by touching textured surfaces.

---

## Table of Contents
1. [Introduction](#introduction)
2. [Methodology](#methodology)
3. [Key Findings](#key-findings)
4. [Conclusion](#conclusion)

---

## Introduction

Visualizing data is a crucial step in building machine learning systems. It helps identify patterns and informs better feature engineering. This task involves:
- Classifying sounds captured by a microphone during interactions with textured surfaces.
- Transforming sound signals into feature vectors using techniques like FFT, DCT, and Cepstrum.
- Evaluating the effectiveness of feature-window combinations through clustering using various visualization methods (PCA, t-SNE, UMAP, etc.).

---

## Methodology

### Classifier
A simple K-Nearest Neighbors (KNN) algorithm was used, with K set to 7. The algorithm predicts classes based on the proximity of data points in the feature space.

### Feature Selection Parameters
Key parameters for feature extraction:
- **Window Size**: Duration of each data segment.
- **Step Size**: Overlap between windows.
- **Decimation**: Reduction in feature vector length.
- **Window Function**: Boxcar, Hann, Hamming, or Blackman-Harris.
- **Feature Functions**: FFT, DCT, Cepstrum, and others.

### Visualization Techniques
- **PCA**: Principal Component Analysis.
- **t-SNE**: t-distributed Stochastic Neighbor Embedding.
- **UMAP**: Uniform Manifold Approximation and Projection.
- **Isomap**: Captures local and global data structure.

---

## Key Findings

| Finding                                | Description                                                                 |
|----------------------------------------|-----------------------------------------------------------------------------|
| **Unclustered Graphs**                 | Poor clustering correlates with low model accuracy.                         |
| **Well-Clustered Graphs**              | Not always indicative of the highest accuracy.                              |
| **Semi-Clustered Graphs**              | Often lead to the best accuracy results.                                    |
| **Best Feature Function**              | Cepstrum delivered the highest accuracy overall.                            |
| **Best Window Function**               | Boxcar consistently outperformed other window functions.                    |
| **Best Visualization Technique**       | t-SNE effectively captured class separations and correlated with accuracy.  |

---

## Conclusion

This project highlights the importance of visualizations in feature engineering and model optimization. Semi-clustered visualizations with t-SNE and the use of Cepstrum features provided the best results for this dataset.

